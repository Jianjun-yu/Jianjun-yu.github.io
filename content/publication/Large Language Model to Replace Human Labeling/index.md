---
title: 'Large Language Model to Replace Human Labeling'

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here
# and it will be replaced with their full name and linked to their profile.
authors:
  - Rongxing Ouyang
  - Jianjun Yu
author_notes:
- "First Author"
- "Corresponding Author"

date: '2023-07-08T00:00:00Z'
doi: ''

# Schedule page publish date (NOT publication's date).
publishDate: ''

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ['3']

# Publication name and optional abbreviated publication name.
publication: 
publication_short: 

abstract: Coding in content analysis is costly and fragile. Scholars use quality control indexes, crowd-sourcing, and specific algorithms to trade-off costs and quality. Instead, the forth version of generative pre-trained transformer (GPT-4) outperforms its previous versions in various tasks (OpenAI, 2023a) at a reasonable cost. We intend to investigate whether such large language models (LLMs) can replace prior methods. Results show that, GPT-4 with prompt-training (PT) outperforms human-beings and a dedicated model in binary classification of emotion in Chinese. The model yields higher accuracy and f1-score than artificial coding and SKEP (Tian et al., 2020), a pre-trained model without downstream fine-tuning. Additionally, GPT-series models remain competitive inter-coder reliability, and GPT-4 has strong inter-hyper-parameter reliability. Thus, they are accurate, reliable, and economical.



tags: [Large language model, Text as data]
featured: false


# Display this page in the Featured widget?


# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org


# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.



---

