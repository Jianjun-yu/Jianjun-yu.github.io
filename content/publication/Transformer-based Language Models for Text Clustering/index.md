---
title: 'Transformer-based Language Models for Text Clustering'

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here
# and it will be replaced with their full name and linked to their profile.
authors:
  - admin

date: '2023-06-01T00:00:00Z'
doi: ''

# Schedule page publish date (NOT publication's date).
publishDate: ''

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ['2']

# Publication name and optional abbreviated publication name.
publication: 
publication_short: 

abstract: Text clustering based on probabilistic topic models has gained significant attention among political scientists in recent years. However, commonly used probabilistic topic models like Latent Dirichlet Allocation and Structural Topic Models exhibit several limitations that hinder their application. Firstly, these models struggle to accurately classify short texts. Secondly, their performance heavily relies on user decisions regarding text preprocessing and the number of clusters selected. Lastly, model training with large text datasets can be time-consuming. In this paper, I propose a workflow that uses transformer-based language models, such as BERT and GPT, for text clustering. This method surpasses traditional topic models in terms of accuracy and time efficiency. Furthermore, it reduces the impact of user decisions on text preprocessing and facilitates easy comparison of different topic numbers' impact on research outcomes. Given the increasing popularity and application of transformer-based language models like ChatGPT, this paper encourages social scientists to explore how this state-of-the-art technology can enhance their research.

summary: Given the increasing popularity and application of transformer-based language models like ChatGPT and BERT, this paper encourages social scientists to explore how this state-of-the-art technology can enhance their research.

tags: [Text clustering, Topic modeling, Large language model]
featured: true

url_pdf: ''
url_appendix_A: ''
url_appendix_B: ''
# Display this page in the Featured widget?


# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org


# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
  caption: 'Image credit: [**The structure of the base BERT model**]'
  focal_point: ''
  preview_only: false


url_pdf: 'https://github.com/Jianjun-yu/Jianjun_website/blob/main/content/publication/Transformer-based%20Language%20Models%20for%20Text%20Clustering/Transformer_based_Language_Models_for_Text_Clustering.pdf'


---
Appendix_A is an introducation of transformer-based models [appendix_A](https://test/).
Other supplementary[appendixes](https://test/).

